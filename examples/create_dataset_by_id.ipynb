{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e4baf5e",
   "metadata": {},
   "source": [
    "# User defined filter function\n",
    "\n",
    "This notebooks demos how to create a user defined filter function (can be generalized to any function).\n",
    "\n",
    "Here we are trying to create a dataloader for a subset of the Experiment data. This subset is defined either by a list of `frame id intervals`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca5867ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from experanto.intervals import (\n",
    "    TimeInterval,\n",
    "    find_complement_of_interval_array,\n",
    "    uniquefy_interval_array,\n",
    ")\n",
    "\n",
    "valid_keys = ['00003','00005','00006','00009',] # Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "883dbd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experanto.datasets import register_callable\n",
    "from experanto.interpolators import Interpolator\n",
    "\n",
    "@register_callable(\"filter2\")\n",
    "def id2interval(dataset=None, id_list=[], complement=False):\n",
    "    '''Convert a list of IDs to intervals.\n",
    "    Args:\n",
    "        id_list (list): The list of IDs.\n",
    "        complement (bool): If True, return the complement of the intervals.\n",
    "    Returns:\n",
    "        list: A list of intervals.'''\n",
    "\n",
    "    def implementation(device_: Interpolator, \n",
    "                       id_list=id_list, \n",
    "                       dataset=dataset, \n",
    "                       complement=complement):\n",
    "\n",
    "        if not id_list:\n",
    "            return []\n",
    "        \n",
    "        id_list = sorted(id_list)\n",
    "    \n",
    "        meta_path = f\"/data/test_upsampling_without_hamming_30.0Hz/{dataset}/screen/combined_meta.json\"\n",
    "        with open(meta_path, 'rb') as f:\n",
    "            meta = json.load(f)\n",
    "\n",
    "        if complement:\n",
    "            all_ids = set(meta.keys())\n",
    "            used_ids = set(id_list)\n",
    "            complement_ids = sorted(all_ids - used_ids)\n",
    "            return id2interval(dataset=dataset, id_list=complement_ids)(device_)\n",
    "\n",
    "        timestamps = np.load(f\"/data/test_upsampling_without_hamming_30.0Hz/{dataset}/screen/timestamps.npy\")\n",
    "        \n",
    "        intervals = []\n",
    "        for i in range(1, len(id_list)):\n",
    "            start = meta[id_list[i]]['first_frame_idx']\n",
    "            end = start + meta[id_list[i]]['num_frames']\n",
    "            intervals.append(TimeInterval(timestamps[start], timestamps[end]))  # start inclusive, end exclusive\n",
    "        \n",
    "        valid_intervals = uniquefy_interval_array(intervals)\n",
    "        \n",
    "        return valid_intervals\n",
    "        \n",
    "    return implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d187e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TimeInterval(start=1682534632.6021829, end=1682534652.623929),\n",
       " TimeInterval(start=1682534673.1462207, end=1682534683.1570945)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_list = valid_keys\n",
    "dataset = \"dynamic29515-10-12-Video-021a75e56847d574b9acbcc06c675055_30hz\"\n",
    "id2interval(id_list=id_list, dataset=dataset)(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e49fa0b",
   "metadata": {},
   "source": [
    "### ToDo: Use the time intervals to dataloader function to complete the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30343a74-fc8e-4ad5-bead-12e2e5465932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path if needed\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0cefb78-bb2a-4bbe-adf5-a9b71fc77628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "\n",
    "from experanto.datasets import ChunkDataset\n",
    "from experanto.dataloaders import get_multisession_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "200cbcd5-4dee-4945-9809-8e42858183c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  global_sampling_rate: null\n",
      "  global_chunk_size: null\n",
      "  add_behavior_as_channels: false\n",
      "  replace_nans_with_means: false\n",
      "  cache_data: false\n",
      "  out_keys:\n",
      "  - screen\n",
      "  - responses\n",
      "  - eye_tracker\n",
      "  - treadmill\n",
      "  - timestamps\n",
      "  normalize_timestamps: true\n",
      "  modality_config:\n",
      "    screen:\n",
      "      keep_nans: false\n",
      "      sampling_rate: 30\n",
      "      chunk_size: 60\n",
      "      valid_condition:\n",
      "        tier: train\n",
      "      offset: 0\n",
      "      sample_stride: 1\n",
      "      include_blanks: true\n",
      "      transforms:\n",
      "        normalization: normalize\n",
      "        Resize:\n",
      "          _target_: torchvision.transforms.v2.Resize\n",
      "          size:\n",
      "          - 144\n",
      "          - 256\n",
      "      interpolation:\n",
      "        rescale: true\n",
      "        rescale_size:\n",
      "        - 144\n",
      "        - 256\n",
      "    responses:\n",
      "      keep_nans: false\n",
      "      sampling_rate: 8\n",
      "      chunk_size: 16\n",
      "      offset: 0.0\n",
      "      transforms:\n",
      "        normalization: standardize\n",
      "      interpolation:\n",
      "        interpolation_mode: nearest_neighbor\n",
      "      filters:\n",
      "        nan_filter:\n",
      "          __target__: experanto.filters.common_filters.nan_filter\n",
      "          __partial__: true\n",
      "          vicinity: 0.05\n",
      "    eye_tracker:\n",
      "      keep_nans: false\n",
      "      sampling_rate: 30\n",
      "      chunk_size: 60\n",
      "      offset: 0\n",
      "      transforms:\n",
      "        normalization: normalize\n",
      "      interpolation:\n",
      "        interpolation_mode: nearest_neighbor\n",
      "      filters:\n",
      "        nan_filter:\n",
      "          __target__: experanto.filters.common_filters.nan_filter\n",
      "          __partial__: true\n",
      "          vicinity: 0.05\n",
      "    treadmill:\n",
      "      keep_nans: false\n",
      "      sampling_rate: 30\n",
      "      chunk_size: 60\n",
      "      offset: 0\n",
      "      transforms:\n",
      "        normalization: normalize\n",
      "      interpolation:\n",
      "        interpolation_mode: nearest_neighbor\n",
      "      filters:\n",
      "        nan_filter:\n",
      "          __target__: experanto.filters.common_filters.nan_filter\n",
      "          __partial__: true\n",
      "          vicinity: 0.05\n",
      "dataloader:\n",
      "  batch_size: 16\n",
      "  shuffle: true\n",
      "  num_workers: 2\n",
      "  pin_memory: true\n",
      "  drop_last: true\n",
      "  prefetch_factor: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from experanto.configs import DEFAULT_CONFIG as cfg\n",
    "\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0dcabcc-2afc-4b86-8546-e1130f52fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.dataset.modality_config.screen.include_blanks = True\n",
    "cfg.dataset.modality_config.screen.valid_condition = {\"tier\": \"train\"}\n",
    "cfg.dataloader.num_workers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad8b6a2c-9afb-4787-9c9d-fd60a566a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of defining a new function in a different file './my_functions/common_filters.py'\n",
    "cfg.dataset.modality_config.treadmill.filters.nan_filter = {\"__key__\": \"filter1\", \"vicinity\": 0.05}\n",
    "cfg.dataset.modality_config.eye_tracker.filters.nan_filter = {\"__key__\": \"filter1\", \"vicinity\": 0.05}\n",
    "cfg.dataset.modality_config.responses.filters.nan_filter = {\"__key__\": \"filter1\", \"vicinity\": 0.05}\n",
    "\n",
    "# Example of defining a new function in a the jupyter notebook\n",
    "# cfg.dataset.modality_config.treadmill.filters.nan_filter = {\"__key__\": \"filter2\", \"id_list\": id_list, \"dataset\": dataset}\n",
    "# cfg.dataset.modality_config.eye_tracker.filters.nan_filter = {\"__key__\": \"filter2\", \"id_list\": id_list, \"dataset\": dataset}\n",
    "# cfg.dataset.modality_config.responses.filters.nan_filter = {\"__key__\": \"filter2\", \"id_list\": id_list, \"dataset\": dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f3704f5-a72b-4060-aed3-ac9bd94c2e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  global_sampling_rate: null\n",
      "  global_chunk_size: null\n",
      "  add_behavior_as_channels: false\n",
      "  replace_nans_with_means: false\n",
      "  cache_data: false\n",
      "  out_keys:\n",
      "  - screen\n",
      "  - responses\n",
      "  - eye_tracker\n",
      "  - treadmill\n",
      "  - timestamps\n",
      "  normalize_timestamps: true\n",
      "  modality_config:\n",
      "    screen:\n",
      "      keep_nans: false\n",
      "      sampling_rate: 30\n",
      "      chunk_size: 60\n",
      "      valid_condition:\n",
      "        tier: train\n",
      "      offset: 0\n",
      "      sample_stride: 1\n",
      "      include_blanks: true\n",
      "      transforms:\n",
      "        normalization: normalize\n",
      "        Resize:\n",
      "          _target_: torchvision.transforms.v2.Resize\n",
      "          size:\n",
      "          - 144\n",
      "          - 256\n",
      "      interpolation:\n",
      "        rescale: true\n",
      "        rescale_size:\n",
      "        - 144\n",
      "        - 256\n",
      "    responses:\n",
      "      keep_nans: false\n",
      "      sampling_rate: 8\n",
      "      chunk_size: 16\n",
      "      offset: 0.0\n",
      "      transforms:\n",
      "        normalization: standardize\n",
      "      interpolation:\n",
      "        interpolation_mode: nearest_neighbor\n",
      "      filters:\n",
      "        nan_filter:\n",
      "          __key__: filter1\n",
      "          vicinity: 0.05\n",
      "    eye_tracker:\n",
      "      keep_nans: false\n",
      "      sampling_rate: 30\n",
      "      chunk_size: 60\n",
      "      offset: 0\n",
      "      transforms:\n",
      "        normalization: normalize\n",
      "      interpolation:\n",
      "        interpolation_mode: nearest_neighbor\n",
      "      filters:\n",
      "        nan_filter:\n",
      "          __key__: filter1\n",
      "          vicinity: 0.05\n",
      "    treadmill:\n",
      "      keep_nans: false\n",
      "      sampling_rate: 30\n",
      "      chunk_size: 60\n",
      "      offset: 0\n",
      "      transforms:\n",
      "        normalization: normalize\n",
      "      interpolation:\n",
      "        interpolation_mode: nearest_neighbor\n",
      "      filters:\n",
      "        nan_filter:\n",
      "          __key__: filter1\n",
      "          vicinity: 0.05\n",
      "dataloader:\n",
      "  batch_size: 16\n",
      "  shuffle: true\n",
      "  num_workers: 8\n",
      "  pin_memory: true\n",
      "  drop_last: true\n",
      "  prefetch_factor: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f621c3e9-8021-4d7a-9eeb-a650cb4d9776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from experanto.dataloaders import get_multisession_dataloader\n",
    "from my_functions import common_filters\n",
    "\n",
    "paths = [\"/data/test_upsampling_without_hamming_30.0Hz/dynamic29515-10-12-Video-021a75e56847d574b9acbcc06c675055_30hz\"]\n",
    "train_dl = get_multisession_dataloader(paths, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb4f259-7e50-4e96-82e1-116abcf65a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
