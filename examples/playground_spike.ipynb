{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f044711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working...\n"
     ]
    }
   ],
   "source": [
    "print(\"Working...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29bea181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from experanto.interpolators import SpikesInterpolator as Interpolator\n",
    "from experanto.interpolators import Interpolator\n",
    "from experanto.intervals import TimeInterval\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from numba import njit, prange\n",
    "import time\n",
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86efabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "from pathlib import Path\n",
    "\n",
    "# --- THE ENGINE ---\n",
    "# 'parallel=True' allows it to use all CPU cores.\n",
    "@njit(parallel=True, fastmath=True)\n",
    "def fast_count_spikes(all_spikes, indices, window_starts, window_ends, out_counts):\n",
    "    \"\"\"\n",
    "    all_spikes: 1D array\n",
    "    indices: 1D array - start/end of each neuron in all_spikes\n",
    "    window_starts: 1D array - start times for the query\n",
    "    window_ends: 1D array\n",
    "    out_counts: 2D array\n",
    "    \"\"\"\n",
    "    n_batch = len(window_starts)\n",
    "    n_neurons = len(indices) - 1\n",
    "    \n",
    "    # We parallelize the OUTER loop (the batch). \n",
    "    # Or we can parallelize the NEURON loop. \n",
    "    # Since N_Neurons (38k) > Batch (e.g. 128), parallelizing neurons is better.\n",
    "    \n",
    "    for i in prange(n_neurons):\n",
    "        # 1. Get the slice for this neuron\n",
    "        # (This is zero-copy in Numba)\n",
    "        idx_start = indices[i]\n",
    "        idx_end = indices[i+1]\n",
    "        neuron_spikes = all_spikes[idx_start:idx_end]\n",
    "        \n",
    "        # 2. Check all time windows for this neuron\n",
    "        # Since spikes are sorted, we use binary search\n",
    "        for b in range(n_batch):\n",
    "            t0 = window_starts[b]\n",
    "            t1 = window_ends[b]\n",
    "            \n",
    "            # Binary Search\n",
    "            # np.searchsorted is supported natively in Numba\n",
    "            # It finds where t0 and t1 would fit in the sorted array\n",
    "            c_start = np.searchsorted(neuron_spikes, t0)\n",
    "            c_end = np.searchsorted(neuron_spikes, t1)\n",
    "            \n",
    "            out_counts[b, i] = c_end - c_start\n",
    "\n",
    "# --- THE CLASS ---\n",
    "class SpikesInterpolator(Interpolator):\n",
    "    def __init__(\n",
    "            self, \n",
    "            root_folder: str,\n",
    "            cache_data: bool = False,\n",
    "            interpolation_window: float = 0.3,\n",
    "            interpolation_align: str = \"center\",\n",
    "            smoothing_sigma: float = 0.0,\n",
    "            load_to_ram: bool = False,\n",
    "            ):\n",
    "        super().__init__(root_folder)\n",
    "\n",
    "        meta = self.load_meta()\n",
    "\n",
    "        self.start_time = meta.get(\"start_time\", 0)\n",
    "        self.end_time = meta.get(\"end_time\", np.inf)\n",
    "        self.valid_interval = TimeInterval(self.start_time, self.end_time)\n",
    "\n",
    "        self.cache_trials = cache_data\n",
    "        self.interpolation_window = interpolation_window\n",
    "        self.interpolation_align = interpolation_align\n",
    "        self.smoothing_sigma = smoothing_sigma\n",
    "\n",
    "        # Use self.root_folder, defined in the base class\n",
    "        self.dat_path = self.root_folder / \"spikes.npy\"\n",
    "        \n",
    "        # Ensure indices are typed correctly for Numba\n",
    "        self.indices = np.array(meta[\"spike_indices\"]).astype(np.int64)\n",
    "        self.n_signals = len(self.indices) - 1\n",
    "\n",
    "        if load_to_ram:\n",
    "            print(\"Loading spikes to RAM...\")\n",
    "            self.spikes = np.fromfile(self.dat_path, dtype='float64')\n",
    "        else:\n",
    "            self.spikes = np.memmap(self.dat_path, dtype='float64', mode='r')\n",
    "\n",
    "    def interpolate(self, times: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        # 1. Filter for valid times\n",
    "        valid = self.valid_times(times)\n",
    "        valid_times = times[valid]\n",
    "        \n",
    "        # Handle edge case where no times are valid\n",
    "        if len(valid_times) == 0:\n",
    "            return np.empty((0, self.n_signals)), valid\n",
    "\n",
    "        # valid_times += 1e-4\n",
    "\n",
    "        # 2. Prepare boundaries\n",
    "        if self.interpolation_align == \"center\":\n",
    "            starts = valid_times - self.interpolation_window / 2\n",
    "            ends   = valid_times + self.interpolation_window / 2\n",
    "        elif self.interpolation_align == \"left\":\n",
    "            starts = valid_times\n",
    "            ends   = valid_times + self.interpolation_window\n",
    "        elif self.interpolation_align == \"right\":\n",
    "            starts = valid_times - self.interpolation_window\n",
    "            ends   = valid_times\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown alignment mode: {self.interpolation_align}\")\n",
    "\n",
    "        # 3. Prepare Output\n",
    "        # SIZE FIX: Only allocate for the VALID batch size\n",
    "        batch_size = len(valid_times)\n",
    "        counts = np.zeros((batch_size, self.n_signals), dtype=np.float64)\n",
    "        \n",
    "        # 4. Call Numba Engine\n",
    "        fast_count_spikes(self.spikes, self.indices, starts, ends, counts)\n",
    "\n",
    "        # 5. Apply Smoothing (Gaussian Filter)\n",
    "        if self.smoothing_sigma > 0:\n",
    "            # We assume 'times' is a sorted, equidistant sequence.\n",
    "            # If batch_size is 1, smoothing is impossible/no-op.\n",
    "            if batch_size > 1:\n",
    "                # Apply Gaussian filter along the time axis (axis 0)\n",
    "                # Note: sigma is in units of array indices (time steps).\n",
    "                # If your times are 30Hz (33ms) and you want 100ms smoothing,\n",
    "                # sigma should be ~3.\n",
    "                counts = gaussian_filter1d(counts, sigma=self.smoothing_sigma, axis=0)\n",
    "        \n",
    "        # SIGNATURE FIX: Return both data and the mask\n",
    "        return counts, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e54dc21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SpikesInterpolator\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded SpikesInterpolator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "017c7e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. DATA GENERATION & TESTING\n",
    "# ==========================================\n",
    "\n",
    "def create_dummy_data(folder_path, n_neurons=50, duration=100.0, rate=20):\n",
    "    \"\"\"Creates synthetic spike data and metadata.\"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    all_spikes = []\n",
    "    indices = [0]\n",
    "    \n",
    "    for _ in range(n_neurons):\n",
    "        n_spikes = int(duration * rate)\n",
    "        # Random spikes sorted\n",
    "        spikes = np.sort(np.random.uniform(0, duration, n_spikes))\n",
    "        all_spikes.append(spikes)\n",
    "        indices.append(indices[-1] + len(spikes))\n",
    "        \n",
    "    flat_spikes = np.concatenate(all_spikes)\n",
    "    flat_spikes.tofile(folder / \"spikes.npy\")\n",
    "    \n",
    "    meta = {\n",
    "        \"modality\": \"spikes\",\n",
    "        \"n_signals\": n_neurons,\n",
    "        \"spike_indices\": indices,\n",
    "        \"start_time\": 0.0,\n",
    "        \"end_time\": duration,\n",
    "        \"sampling_rate\": 1000.0 \n",
    "    }\n",
    "    with open(folder / \"meta.yml\", \"w\") as f:\n",
    "        yaml.dump(meta, f)\n",
    "        \n",
    "    return all_spikes, flat_spikes, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "480cf8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Environment: /mnt/lustre-grete/tmp/u18196/tmpq5c1k71h\n",
      "\n",
      "============================================================\n",
      "STARTING TESTS: 1000 queries, Window=0.5s\n",
      "============================================================\n",
      "\n",
      ">>> Testing Alignment: CENTER\n",
      "Time: 60.69 ms\n",
      "Speed: 16478 queries/sec\n",
      "Verifying accuracy...\n",
      "SUCCESS: All counts match.\n",
      "\n",
      ">>> Testing Alignment: LEFT\n",
      "Time: 54.74 ms\n",
      "Speed: 18269 queries/sec\n",
      "Verifying accuracy...\n",
      "SUCCESS: All counts match.\n",
      "\n",
      ">>> Testing Alignment: RIGHT\n",
      "Time: 47.27 ms\n",
      "Speed: 21156 queries/sec\n",
      "Verifying accuracy...\n",
      "SUCCESS: All counts match.\n",
      "\n",
      "============================================================\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3. TEST RUNNER (ALIGNMENT + SPEED)\n",
    "# ==========================================\n",
    "\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "try:\n",
    "    print(f\"Test Environment: {temp_dir}\")\n",
    "    gt_spikes_list, gt_flat, gt_indices = create_dummy_data(temp_dir, n_neurons=500, duration=1000.0)\n",
    "    \n",
    "    # Define query times (randomly sampled)\n",
    "    n_queries = 1000\n",
    "    query_times = np.sort(np.random.uniform(1.0, 999.0, size=n_queries))\n",
    "    window_size = 0.5\n",
    "    \n",
    "    # alignments_to_test = [\"center\"]\n",
    "    alignments_to_test = [\"center\", \"left\", \"right\"]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"STARTING TESTS: {n_queries} queries, Window={window_size}s\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for align in alignments_to_test:\n",
    "        print(f\"\\n>>> Testing Alignment: {align.upper()}\")\n",
    "        \n",
    "        # 1. Instantiate\n",
    "        interpolator = SpikesInterpolator(\n",
    "            temp_dir, \n",
    "            interpolation_window=window_size, \n",
    "            interpolation_align=align,\n",
    "            # smoothing_sigma=3.0,\n",
    "        )\n",
    "        \n",
    "        # 2. Run & Time\n",
    "        # Warmup (optional, to compile JIT)\n",
    "        _ = interpolator.interpolate(query_times[:10])\n",
    "        \n",
    "        start_t = time.perf_counter()\n",
    "        counts, valid = interpolator.interpolate(query_times)\n",
    "        end_t = time.perf_counter()\n",
    "        \n",
    "        duration_sec = end_t - start_t\n",
    "        speed_qps = n_queries / duration_sec\n",
    "        \n",
    "        print(f\"Time: {duration_sec*1000:.2f} ms\")\n",
    "        print(f\"Speed: {speed_qps:.0f} queries/sec\")\n",
    "        \n",
    "        # 3. Verify Correctness\n",
    "        print(\"Verifying accuracy...\")\n",
    "        errors = 0\n",
    "        big_errors = 0\n",
    "        total_checks = 0\n",
    "        \n",
    "        for t_idx, t in enumerate(query_times):\n",
    "            # Adjust ground truth logic based on alignment\n",
    "            if align == \"center\":\n",
    "                t_start = t - window_size/2\n",
    "                t_end   = t + window_size/2\n",
    "            elif align == \"left\":\n",
    "                t_start = t\n",
    "                t_end   = t + window_size\n",
    "            elif align == \"right\":\n",
    "                t_start = t - window_size\n",
    "                t_end   = t\n",
    "                \n",
    "            for n_idx in range(len(gt_spikes_list)):\n",
    "                total_checks += 1\n",
    "                neuron_spikes = gt_spikes_list[n_idx]\n",
    "                # Ground truth count\n",
    "                manual_count = np.sum((neuron_spikes >= t_start) & (neuron_spikes < t_end))\n",
    "                \n",
    "                numba_count = counts[t_idx, n_idx]\n",
    "                \n",
    "                if manual_count != numba_count:\n",
    "                    # Print only the first error to avoid spamming\n",
    "                    if errors%10 == 0:\n",
    "                        print(f\"Mismatch at time {t:.2f}, neuron {n_idx}: Expected {manual_count}, got {numba_count}\")\n",
    "                    errors += 1\n",
    "                    if abs(manual_count - numba_count) > 1:\n",
    "                        big_errors += 1\n",
    "\n",
    "        if errors == 0:\n",
    "            print(\"SUCCESS: All counts match.\")\n",
    "        else:\n",
    "            print(f\"FAILED: {errors}/{total_checks} mismatches found. {errors/total_checks*100:.2f}%\")\n",
    "            print(f\"Large Errors (>1 count difference): {big_errors}\")\n",
    "\n",
    "finally:\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Cleanup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8394a267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING TESTS: 1000 queries, Window=0.5s\n",
      "============================================================\n",
      "\n",
      ">>> Testing Alignment: CENTER\n",
      "Time: 500.49 ms\n",
      "Speed: 1998 queries/sec\n",
      "\n",
      ">>> Testing Alignment: LEFT\n",
      "Time: 444.03 ms\n",
      "Speed: 2252 queries/sec\n",
      "\n",
      ">>> Testing Alignment: RIGHT\n",
      "Time: 470.41 ms\n",
      "Speed: 2126 queries/sec\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3. TEST RUNNER (ALIGNMENT + SPEED)\n",
    "# ==========================================\n",
    "temp_dir = \"/mnt/vast-nhr/projects/nix00014/goirik/mozaik-models/experanto/data\"\n",
    "duration=1000.0\n",
    "\n",
    "# Define query times (randomly sampled)\n",
    "n_queries = 1000\n",
    "query_times = np.sort(np.random.uniform(1.0, duration, size=n_queries))\n",
    "window_size = 0.5\n",
    "\n",
    "# alignments_to_test = [\"center\"]\n",
    "alignments_to_test = [\"center\", \"left\", \"right\"]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"STARTING TESTS: {n_queries} queries, Window={window_size}s\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for align in alignments_to_test:\n",
    "    print(f\"\\n>>> Testing Alignment: {align.upper()}\")\n",
    "    \n",
    "    # 1. Instantiate\n",
    "    interpolator = SpikesInterpolator(\n",
    "        temp_dir, \n",
    "        interpolation_window=window_size, \n",
    "        interpolation_align=align\n",
    "    )\n",
    "    \n",
    "    # 2. Run & Time\n",
    "    # Warmup (optional, to compile JIT)\n",
    "    _ = interpolator.interpolate(query_times[:10])\n",
    "    \n",
    "    start_t = time.perf_counter()\n",
    "    counts, valid = interpolator.interpolate(query_times)\n",
    "    end_t = time.perf_counter()\n",
    "    \n",
    "    duration_sec = end_t - start_t\n",
    "    speed_qps = n_queries / duration_sec\n",
    "    \n",
    "    print(f\"Time: {duration_sec*1000:.2f} ms\")\n",
    "    print(f\"Speed: {speed_qps:.0f} queries/sec\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experanto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
